{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building your first AzureML Spark web service\n",
    "\n",
    "In this tutorial, we will walk you through loading a dataset, exploring\n",
    "its features, training a model on the dataset, and then publishing a\n",
    "realtime scoring API for the model.\n",
    "\n",
    "First, let's read in the Boston Housing Price dataset. We have placed a copy in your azureml/datasets folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import Azure ML API SDK. The SDK is installed implicitly with the latest\n",
    "# version of the CLI in your default python environment\n",
    "from azure.ml.api.schema.dataTypes import DataTypes\n",
    "from azure.ml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azure.ml.api.realtime.services import generate_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the housing price dataset\n",
    "df2 = spark.read.csv(\"../datasets/housing.csv\", header=True, inferSchema=True)\n",
    "df2.show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train your model\n",
    "\n",
    "Using Spark's ML library, we can train a gradient boosted tree regressor for our data to produce a model that can predict median values of houses in Boston. Once we have trained the model, we can then evaluate it for quality using the root mean squared error metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Train a boosted decision tree regressor\n",
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "import numpy as np\n",
    "formula = RFormula(formula=\"MEDV~.\")\n",
    "gbt = GBTRegressor()\n",
    "pipeline = Pipeline(stages=[formula, gbt]).fit(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 error = 0.9776138989970254\n"
     ]
    }
   ],
   "source": [
    "# Evaluate scores\n",
    "scores = pipeline.transform(df2)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "print(\"R^2 error = \" + str(RegressionEvaluator(metricName=\"r2\").evaluate(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Save your model and schema\n",
    "\n",
    "Once you have a model that performs well, you can package it into a scoring service. To prepare for this, save your model and dataset schema locally first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "pipeline.write().overwrite().save(\"housing.model\")\n",
    "print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Authoring a Realtime Web service\n",
    "\n",
    "In this section, we show you how to author a realtime web service that scores the model you saved above. \n",
    "\n",
    "### 1. Define ```init``` and ```run``` and create the ```score.py``` file\n",
    "\n",
    "We start by defining our ```init``` and ```run``` functions in the cell below. Then write them to the score.py file. This file will load the model, perform the prediction, and return the result.\n",
    "\n",
    "The ```init``` function initializes your web service, loading in any data or models that you need to score your inputs. In the example below, we load in the trained model. This command is run when the Docker contianer containing your service initializes.\n",
    "\n",
    "The ```run``` function defines what is executed on a scoring call. In our simple example, we simply load in the input as a data frame, and run our pipeline on the input, and return the prediction. \n",
    "\n",
    "The %%writefile command will save the score.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "def init():\n",
    "    # read in the model file\n",
    "    from pyspark.ml import PipelineModel\n",
    "    global pipeline\n",
    "    pipeline = PipelineModel.load(\"housing.model\")\n",
    "    \n",
    "def run(input_df):\n",
    "    response = ''\n",
    "    \n",
    "    try:\n",
    "        #Get prediction results for the dataframe\n",
    "        score = pipeline.transform(input_df)\n",
    "        predictions = score.collect()\n",
    "\n",
    "        #Get each scored result\n",
    "        for pred in predictions:\n",
    "            response += str(pred['prediction']) + \",\"\n",
    "        # Remove the last comma\n",
    "        response = response[:-1]\n",
    "    except Exception as e:\n",
    "        return (str(e))\n",
    "    \n",
    "    # Return results\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Schema\n",
    "\n",
    "Create a schema for the input to the web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define the input data frame\n",
    "inputs = {\"input_df\": SampleDefinition(DataTypes.SPARK, df2.drop(\"MEDV\"))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create schema file\n",
    "\n",
    "Generate the schema file. This will be used to create a Swagger file for your web service which can be used to discover its input and sample data when calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': {'input_df': {'internal': {'fields': [{'metadata': {},\n",
       "      'name': 'CRIM',\n",
       "      'nullable': True,\n",
       "      'type': 'double'},\n",
       "     {'metadata': {}, 'name': 'ZN', 'nullable': True, 'type': 'double'},\n",
       "     {'metadata': {}, 'name': 'INDUS', 'nullable': True, 'type': 'double'},\n",
       "     {'metadata': {}, 'name': 'CHAS', 'nullable': True, 'type': 'integer'},\n",
       "     {'metadata': {}, 'name': 'NOX', 'nullable': True, 'type': 'double'},\n",
       "     {'metadata': {}, 'name': 'RM', 'nullable': True, 'type': 'double'},\n",
       "     {'metadata': {}, 'name': 'AGE', 'nullable': True, 'type': 'double'},\n",
       "     {'metadata': {}, 'name': 'DIS', 'nullable': True, 'type': 'double'},\n",
       "     {'metadata': {}, 'name': 'RAD', 'nullable': True, 'type': 'integer'},\n",
       "     {'metadata': {}, 'name': 'TAX', 'nullable': True, 'type': 'integer'},\n",
       "     {'metadata': {}, 'name': 'PTRATIO', 'nullable': True, 'type': 'double'},\n",
       "     {'metadata': {}, 'name': 'LSTAT', 'nullable': True, 'type': 'double'}],\n",
       "    'type': 'struct'},\n",
       "   'swagger': {'example': [{'AGE': 65.2,\n",
       "      'CHAS': 0,\n",
       "      'CRIM': 0.00632,\n",
       "      'DIS': 4.09,\n",
       "      'INDUS': 2.31,\n",
       "      'LSTAT': 4.98,\n",
       "      'NOX': 0.538,\n",
       "      'PTRATIO': 15.3,\n",
       "      'RAD': 1,\n",
       "      'RM': 6.575,\n",
       "      'TAX': 296,\n",
       "      'ZN': 18.0},\n",
       "     {'AGE': 78.9,\n",
       "      'CHAS': 0,\n",
       "      'CRIM': 0.02731,\n",
       "      'DIS': 4.9671,\n",
       "      'INDUS': 7.07,\n",
       "      'LSTAT': 9.14,\n",
       "      'NOX': 0.469,\n",
       "      'PTRATIO': 17.8,\n",
       "      'RAD': 2,\n",
       "      'RM': 6.421,\n",
       "      'TAX': 242,\n",
       "      'ZN': 0.0},\n",
       "     {'AGE': 61.1,\n",
       "      'CHAS': 0,\n",
       "      'CRIM': 0.02729,\n",
       "      'DIS': 4.9671,\n",
       "      'INDUS': 7.07,\n",
       "      'LSTAT': 4.03,\n",
       "      'NOX': 0.469,\n",
       "      'PTRATIO': 17.8,\n",
       "      'RAD': 2,\n",
       "      'RM': 7.185,\n",
       "      'TAX': 242,\n",
       "      'ZN': 0.0}],\n",
       "    'items': {'properties': {'AGE': {'format': 'double', 'type': 'number'},\n",
       "      'CHAS': {'format': 'int32', 'type': 'integer'},\n",
       "      'CRIM': {'format': 'double', 'type': 'number'},\n",
       "      'DIS': {'format': 'double', 'type': 'number'},\n",
       "      'INDUS': {'format': 'double', 'type': 'number'},\n",
       "      'LSTAT': {'format': 'double', 'type': 'number'},\n",
       "      'NOX': {'format': 'double', 'type': 'number'},\n",
       "      'PTRATIO': {'format': 'double', 'type': 'number'},\n",
       "      'RAD': {'format': 'int32', 'type': 'integer'},\n",
       "      'RM': {'format': 'double', 'type': 'number'},\n",
       "      'TAX': {'format': 'int32', 'type': 'integer'},\n",
       "      'ZN': {'format': 'double', 'type': 'number'}},\n",
       "     'type': 'object'},\n",
       "    'type': 'array'},\n",
       "   'type': 2}}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import score\n",
    "generate_schema(run_func=score.run, inputs=inputs, filepath='service_schema.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. Test ```init``` and ```run```\n",
    "\n",
    "We can then test the ```init``` and ```run``` functions right here in the notebook, before we decide to actually publish a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.687468518340438,19.21096852021207,24.48585095195574\n"
     ]
    }
   ],
   "source": [
    "# Create the sample input dataframe\n",
    "input_data = [[0.00632, 18.0, 2.31, 0, 0.538, 6.575, 65.2, 4.09, 1, 296, 15.3, 4.98, 24.0],[0.00632, 59.0, 2.31, 0, 0.538, 6.575, 65.2, 4.09, 1, 296, 15.3, 4.98, 24.0],[0.00332, 76.0, 2.31, 0, 0.538, 6.575, 65.2, 4.09, 1, 296, 15.3, 4.98, 12.0]]\n",
    "df = spark.createDataFrame(input_data, [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"])\n",
    "\n",
    "#Call the run function to score using the model\n",
    "score.init() #Score file imported above\n",
    "print(score.run(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 4. Use the CLI to deploy and manage your web service\n",
    "\n",
    "#### Pre-requisites\n",
    "\n",
    "Use the following commands to set up an environment and account to run the web service. For more info, see the Getting Started Guide and the CLI Command Reference. You can use -h flag at the end of the commands for command help.\n",
    "\n",
    "* Create the environment (you need to do this once per environment e.g. dev or prod)\n",
    "\n",
    "```\n",
    "az ml env setup -c -n <yourclustername> --location <e.g. eastus2>\n",
    "```\n",
    "* Create a Model Management account (one time setup)\n",
    "\n",
    "```\n",
    "az ml account modelmanagement create --location <e.g. eastus2> -n <your-new-acctname> -g <yourresourcegroupname> --sku-capacity 1 --sku-name S1\n",
    "```\n",
    "\n",
    "*  Set the Model Management account\n",
    "\n",
    "```\n",
    "az ml account modelmanagement set -n <youracctname> -g <yourresourcegroupname>\n",
    "```\n",
    "\n",
    "*  Set the environment. The cluster name is the name used in step 1 above. The resource group name was the output of the same process and would be in the command window when the setup process is completed.\n",
    "\n",
    "```\n",
    "az ml env set -n <yourclustername> -g <yourresourcegroupname>\n",
    "```\n",
    "\n",
    "#### Deploy your web service\n",
    "\n",
    "Switch to a bash shell, and run the following commands to deploy your service and run it.\n",
    "\n",
    "Enter the path where the notebook and other files are saved. Your actual path may be different from this example.\n",
    "```\n",
    "cd ~/notebooks/azureml/spark/realtime/\n",
    "```\n",
    "This assumes that you saved your model locally.\n",
    "```\n",
    "az ml service create realtime --model-file housing.model -f score.py -n housingservice -s service_schema.json -r spark-py\n",
    "```\n",
    "This command will return the sample run command with sample data. \n",
    "You can get the Service Id from the output of the create command above.\n",
    "```\n",
    "az ml service show realtime -i <yourserviceid>\n",
    "```\n",
    "Call the web service to get a prediction\n",
    "```\n",
    "az ml service run realtime -i <yourserviceid> -d \"{\\\"input_df\\\": [{\\\"CRIM\\\": 0.00632, \\\"RM\\\": 6.575, \\\"TAX\\\": 296, \\\"NOX\\\": 0.538, \\\"PTRATIO\\\": 15.3, \\\"LSTAT\\\": 4.98, \\\"CHAS\\\": 0, \\\"DIS\\\": 4.09, \\\"INDUS\\\": 2.31, \\\"RAD\\\": 1, \\\"ZN\\\": 18.0, \\\"AGE\\\": 65.2}]}\"\n",
    "```\n",
    "Prediction result:\n",
    "\n",
    "{'result': '24.27495913312397'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
