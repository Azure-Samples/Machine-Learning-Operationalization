{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# How to create a real-time web service for a Spark model on Azure\n",
    "\n",
    "Before running the tutorial, you must configure your DSVM as specified in the README on the [Machine Learning Operationalization](https://aka.ms/o16ncli) GitHub repo. If you have previously configured your DSVM, you may want to check the GitHub repo to ensure that you are using the most recent instructions\n",
    "\n",
    "In the tutorial, we will walk you through loading a dataset, exploring\n",
    "its features, training a model on the dataset, and then publishing a\n",
    "realtime scoring API for the model.\n",
    "\n",
    "First, read in the Boston Housing Price dataset. This dataset is publicly available at https://archive.ics.uci.edu/ml/datasets/Housing. We have placed a copy in your ```azureml/datasets``` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import Azure ML API SDK. The SDK is installed implicitly with the latest\n",
    "# version of the CLI in your default python environment\n",
    "from azure.ml.api.schema.dataTypes import DataTypes\n",
    "from azure.ml.api.schema.sampleDefinition import SampleDefinition\n",
    "from azure.ml.api.realtime.services import prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the housing price dataset\n",
    "df2 = spark.read.csv(\"datasets/housing.csv\", header=True, inferSchema=True)\n",
    "df2.show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train your model\n",
    "\n",
    "Using Spark's ML library, we can train a gradient boosted tree regressor for our data to produce a model that can predict median values of houses in Boston. Once you have trained the model, you can evaluate it for quality using the root mean squared error metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Train a boosted decision tree regressor\n",
    "from pyspark.ml.feature import RFormula\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "import numpy as np\n",
    "formula = RFormula(formula=\"MEDV~.\")\n",
    "gbt = GBTRegressor()\n",
    "pipeline = Pipeline(stages=[formula, gbt]).fit(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluate scores\n",
    "scores = pipeline.transform(df2)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "print \"R^2 error =\", RegressionEvaluator(metricName=\"r2\").evaluate(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Save your model\n",
    "\n",
    "Once you have a model that performs well, you can package it into a scoring service. To prepare for this, save your model locally first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "pipeline.write().overwrite().save(\"housing.model\")\n",
    "print \"Model saved\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Authoring a Realtime Web Service\n",
    "\n",
    "In this section, you how author a realtime web service that scores the model you saved above. \n",
    "\n",
    "### Define ```init``` and ```run```\n",
    "\n",
    "Start by defining your ```init``` and ```run``` functions in the cell below. \n",
    "\n",
    "The ```init``` function initializes the web service, loading in any data or models that it needs to score your inputs. In the example below, it loads in the trained model and the schema of your dataset.\n",
    "\n",
    "The ```run``` function defines what is executed on a scoring call. In this simple example, the service loads the json input as a data frame and runs the pipeline on the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Prepare the web service definition by authoring\n",
    "# init() and run() functions. \n",
    "# User written init function should mainly focus on loading the model(s) now. Schema loading is done in generated code\n",
    "def init():\n",
    "    from pyspark.ml import PipelineModel\n",
    "    global pipeline\n",
    "    pipeline = PipelineModel.load(\"housing.model\")\n",
    "\n",
    "def run(input_df):\n",
    "    score = pipeline.transform(input_df)\n",
    "    return score.collect()[0]['prediction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create a schema file \n",
    "\n",
    "To generate a schema for the inputs (and outputs for rich swagger), You define a map of input names to input sample data. The input name must match exactly with the names of the arguments for the run function. For samples use the data structures you created and used for testing the model after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inputs = {\"input_df\": SampleDefinition(DataTypes.SPARK, df2.drop(\"MEDV\"))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create the driver and schema files\n",
    "\n",
    "Finally, we put it all of this together by calling the prepare function with the init, run, and inputs (and/or outputs) definitions.\n",
    "\n",
    "This creates a folder named *output_{timestamp}* in the current working directory (by default, or can use the drop_folder param to override that) which contais the driver program named *main.py* and a schema file named *service_schema.json*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prepare(run_func=run, init_func=init, input_types=inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Test ```init``` and ```run```\n",
    "\n",
    "Before publishing the web service, you can test the init and run functions in the notebook by running the the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_data = [[0.00632, 18.0, 2.31, 0, 0.538, 6.575, 65.2, 4.09, 1, 296, 15.3, 4.98, 24.0]]\n",
    "df = spark.createDataFrame(input_data, [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"])\n",
    "init()\n",
    "print(run(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Use the CLI to deploy and manage your web services\n",
    "\n",
    "SSH into the DSVM and run the following commands to deploy your service locally.\n",
    "\n",
    "Set the environment variables, either from the command line or from a script, that you generated when you setup your DSVM. \n",
    "\n",
    "Change to azureml folder containing the realtime notebook.\n",
    "\n",
    "```\n",
    "cd ~/notebooks/azureml/realtime\n",
    "```\n",
    "Next, using the driver and schema files that are output to the *output\\_{timestamp}* folder, run the following commands to create the web service:\n",
    "\n",
    "```\n",
    "az ml env local\n",
    "az ml service create realtime -f main.py -m housing.model -s service_schema.json -n mytestapp -r spark-py -v\n",
    "```\n",
    "\n",
    "To create and run the web service on the ACS cluster, change to the cluster mode and rerun the service creation command:\n",
    "\n",
    "```\n",
    "az ml env cluster\n",
    "az ml service create realtime -f main.py -m housing.model -s service_schema.json -n mytestapp -r spark-py -v\n",
    "```\n",
    "\n",
    "To test the local web service, run the following command with a sample data input:\n",
    "\n",
    "Linux\n",
    "\n",
    "```\n",
    "az ml service run realtime -n mytestapp -d '\"{\\\"input_df\\\": [{\\\"CRIM\\\": 0.00632, \\\"RM\\\": 6.575, \\\"TAX\\\": 296, \\\"NOX\\\": 0.538, \\\"PTRATIO\\\": 15.3, \\\"LSTAT\\\": 4.98, \\\"CHAS\\\": 0, \\\"DIS\\\": 4.09, \\\"INDUS\\\": 2.31, \\\"RAD\\\": 1, \\\"ZN\\\": 18.0, \\\"AGE\\\": 65.2}, {\\\"CRIM\\\": 0.02731, \\\"RM\\\": 6.421, \\\"TAX\\\": 242, \\\"NOX\\\": 0.469, \\\"PTRATIO\\\": 17.8, \\\"LSTAT\\\": 9.14, \\\"CHAS\\\": 0, \\\"DIS\\\": 4.9671, \\\"INDUS\\\": 7.07, \\\"RAD\\\": 2, \\\"ZN\\\": 0.0, \\\"AGE\\\": 78.9}, {\\\"CRIM\\\": 0.02729, \\\"RM\\\": 7.185, \\\"TAX\\\": 242, \\\"NOX\\\": 0.469, \\\"PTRATIO\\\": 17.8, \\\"LSTAT\\\": 4.03, \\\"CHAS\\\": 0, \\\"DIS\\\": 4.9671, \\\"INDUS\\\": 7.07, \\\"RAD\\\": 2, \\\"ZN\\\": 0.0, \\\"AGE\\\": 61.1}]}\"'\n",
    "```\n",
    "\n",
    "Windows\n",
    "\n",
    "```\n",
    "az ml service run realtime -n mytestapp1 -d \"\\\"{\\\"input_df\\\": [{\\\"CRIM\\\": 0.00632, \\\"RM\\\": 6.575, \\\"TAX\\\": 296, \\\"NOX\\\": 0.538, \\\"PTRATIO\\\": 15.3, \\\"LSTAT\\\": 4.98, \\\"CHAS\\\": 0, \\\"DIS\\\": 4.09, \\\"INDUS\\\": 2.31, \\\"RAD\\\": 1, \\\"ZN\\\": 18.0, \\\"AGE\\\": 65.2}, {\\\"CRIM\\\": 0.02731, \\\"RM\\\": 6.421, \\\"TAX\\\": 242, \\\"NOX\\\": 0.469, \\\"PTRATIO\\\": 17.8, \\\"LSTAT\\\": 9.14, \\\"CHAS\\\": 0, \\\"DIS\\\": 4.9671, \\\"INDUS\\\": 7.07, \\\"RAD\\\": 2, \\\"ZN\\\": 0.0, \\\"AGE\\\": 78.9}, {\\\"CRIM\\\": 0.02729, \\\"RM\\\": 7.185, \\\"TAX\\\": 242, \\\"NOX\\\": 0.469, \\\"PTRATIO\\\": 17.8, \\\"LSTAT\\\": 4.03, \\\"CHAS\\\": 0, \\\"DIS\\\": 4.9671, \\\"INDUS\\\": 7.07, \\\"RAD\\\": 2, \\\"ZN\\\": 0.0, \\\"AGE\\\": 61.1}]}\\\"\"\n",
    "\n",
    "```\n",
    "\n",
    "You can retrieve the swagger document using the following command\n",
    "\n",
    "```\n",
    "curl http://127.0.0.1:<portNumber>/swagger.json\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2 Spark - local",
   "language": "python",
   "name": "spark-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
